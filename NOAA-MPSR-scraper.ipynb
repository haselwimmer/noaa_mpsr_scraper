{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Marine Pollution Surveillance Reports Web Scraper\n",
    "\n",
    "This code scrapes salient information from the linked .txt files of the NOAA MSPR service website and saves the output as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import re\n",
    "import copy\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from twilio.rest import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to convert DMS to DD; adapted from \n",
    "# https://gist.github.com/chrisjsimpson/076a82b51e8540a117e8aa5e793d06ec\n",
    "def dms2dec(dms_str):\n",
    "    \"\"\"Converts coordinates in Degrees Minutes \n",
    "    Seconds (DMS) to Decimal Degrees (DD)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dms_str : str\n",
    "        Coordinates in DMS\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returned_data : str\n",
    "        Coordinates in DD\n",
    "    \"\"\"\n",
    "\n",
    "    dms_str = re.sub(r'\\s', '', dms_str)\n",
    "    sign = -1 if re.search('[swSW]', dms_str) else 1\n",
    "    numbers = [*filter(len, re.split('\\D+', dms_str, maxsplit=4))]\n",
    "    degree = numbers[0]\n",
    "    minute = numbers[1] if len(numbers) >= 2 else '0'\n",
    "    second = numbers[2] if len(numbers) >= 3 else '0'\n",
    "    frac_seconds = numbers[3] if len(numbers) >= 4 else '0'\n",
    "    second += \".\" + frac_seconds\n",
    "    return sign * (int(degree) + float(minute) / 60 + float(second) / 3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile links to NOAA MPSR reports from yearly report pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL's for each years web pages from NOAA MPSR\n",
    "urls = ['https://www.ospo.noaa.gov/Products/ocean/marinepollution/2020_archive.html',\n",
    "        'https://www.ospo.noaa.gov/Products/ocean/marinepollution/2019_archive.html',\n",
    "        'https://www.ospo.noaa.gov/Products/ocean/marinepollution/2018_archive.html']\n",
    "\n",
    "#Iterate through web pages for each year and extract links to 'txt' files for each report\n",
    "i=0\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #Get the links to MPSR reports\n",
    "    if i == 0:\n",
    "        files = soup.findAll('a', string='txt')\n",
    "        i = i + 1\n",
    "    else:\n",
    "        files = files + soup.findAll('a', string='txt')\n",
    "\n",
    "#TESTING - create subset of specific files using list comprehension\n",
    "#files = [files[i] for i in (1, 50, 500, 600)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('noaa_mpsr.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "#Drop table for testing purposes\n",
    "#cur.execute('DROP TABLE IF EXISTS reports')\n",
    "\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS reports (url TEXT UNIQUE, data TEXT)')\n",
    "    \n",
    "new_reports = 0\n",
    "    \n",
    "for file in files:\n",
    "    \n",
    "    url = 'https://www.ospo.noaa.gov' + file['href']\n",
    "    \n",
    "    #Check and skip reports in HTML format\n",
    "    if '.html' in url: \n",
    "        #print('.html file skipped')\n",
    "        continue\n",
    "\n",
    "    #Check if row is already in database and add if not\n",
    "    cur.execute('SELECT url FROM reports WHERE url= ?', (url, ) )\n",
    "    msg = cur.fetchone()\n",
    "    \n",
    "    if not msg:\n",
    "        \n",
    "        report_data = requests.get(url)\n",
    "        report_data = report_data.text\n",
    "        cur.execute('''INSERT OR IGNORE INTO reports (url, data) VALUES ( ?, ? )''', ( url, report_data) )\n",
    "        conn.commit()\n",
    "        \n",
    "        new_reports = new_reports + 1\n",
    "        \n",
    "        #Sleep between file downloads to avoid being blocked\n",
    "        time.sleep(.10)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print('There are', new_reports, 'new NOAA MPSR reports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process each report and save into Geojson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_reports >= 1:\n",
    "\n",
    "    #Setup dictionary to store MPSR reports data with defined keys\n",
    "    data_template = {'SOURCE': None,\n",
    "                     'REPORT DATE': None, \n",
    "                     'REPORT TIME': None,\n",
    "                     'REGION': None,\n",
    "                     'SUB REGION': None,\n",
    "                     'AREA/BLOCK': None,\n",
    "                     'IMAGE DATE': None,\n",
    "                     'IMAGE TIME': None,\n",
    "                     'DATA SOURCE': None,\n",
    "                     'MODE': None,\n",
    "                     'RESOLUTION': None,\n",
    "                     'LOCATION': None,\n",
    "                     'AREA': None,\n",
    "                     'CONFIDENCE': None,\n",
    "                     'REMARKS': None,\n",
    "                     'UNCERTAINTIES': None,\n",
    "                     'ANALYST': None,                 \n",
    "                     'ERMA': None}\n",
    "\n",
    "    #Process each report\n",
    "    #Filler value\n",
    "    filler = 'Null'\n",
    "\n",
    "    #Get the data from the database\n",
    "    cur.execute('SELECT * FROM reports')\n",
    "    files = cur.fetchall()\n",
    "\n",
    "    report_count = 1\n",
    "\n",
    "    #Process each report in the database\n",
    "    for file in files:\n",
    "\n",
    "        #Create new blank temporary dictionary from template\n",
    "        data_temp = copy.deepcopy(data_template)\n",
    "\n",
    "        #Split txt file by carriage return\n",
    "        lines = file[1].splitlines()\n",
    "\n",
    "        #For each file populate each key/value pair with a placeholder value to ensure arrays are all the same size\n",
    "        for key in data_temp: data_temp[key] = [filler]        \n",
    "\n",
    "        #Get source URL from database\n",
    "        data_temp['SOURCE'] = file[0]\n",
    "\n",
    "        #Process each line\n",
    "        line_count = 0\n",
    "        for line in lines:\n",
    "\n",
    "            #Check for non standard lines and blank lines\n",
    "            if '***NOTE' in line: \n",
    "                continue\n",
    "            if not line: \n",
    "                continue\n",
    "\n",
    "            #Split lines by colon\n",
    "            line_data = line.split(':', 1)\n",
    "            label = line_data[0]        \n",
    "\n",
    "            #Check if the current label is a dictionary key\n",
    "            if label in data_temp:\n",
    "                #try and except because some files have poorly formed data\n",
    "                try:\n",
    "                    row_data = line_data[1]\n",
    "                    row_data = row_data.strip()\n",
    "                    data_temp[label] = [row_data]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        #Compile results into dataframe\n",
    "        if report_count is 1:\n",
    "            data_table_df = pd.DataFrame.from_dict(data_temp)\n",
    "        else:\n",
    "            data_temp_df = pd.DataFrame.from_dict(data_temp)\n",
    "            data_table_df = data_table_df.append(data_temp_df, ignore_index=True)\n",
    "\n",
    "        #Print status message\n",
    "        #print(report_count, 'of', len(files), 'reports processed')\n",
    "\n",
    "        report_count = report_count + 1\n",
    "\n",
    "\n",
    "    #Re-format LOCATION field into LAT and LON\n",
    "\n",
    "    #Create new data frame splitting LOCATION into LAT/LON\n",
    "    location = data_table_df['LOCATION'].str.split(\"/\", n = 1, expand = True)\n",
    "\n",
    "    #Merge split LAT/LON back into data_table_df and drop old field\n",
    "    data_table_df['LAT'] = location[0]\n",
    "    data_table_df['LON'] = location[1]\n",
    "    data_table_df.drop(columns =['LOCATION'], inplace = True)\n",
    "\n",
    "    #Convert DMS to DD\n",
    "    data_table_df['LAT'] = data_table_df['LAT'].apply(dms2dec)\n",
    "    data_table_df['LON'] = data_table_df['LON'].apply(dms2dec)\n",
    "\n",
    "    #Convert to geodataframe\n",
    "    data_table_gdf = geopandas.GeoDataFrame(\n",
    "        data_table_df, geometry=geopandas.points_from_xy(data_table_df.LON, data_table_df.LAT))\n",
    "\n",
    "    #data_table_gdf.style\n",
    "\n",
    "    #Write to Geojson file\n",
    "    data_table_gdf.to_file(\"NOAA_MPSR.geojson\", driver='GeoJSON')\n",
    "\n",
    "else:\n",
    "    print('No new reports, aborting creation of Geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send SMS message to notify when updates have been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Account SID from twilio.com/console\n",
    "account_sid = ''\n",
    "\n",
    "# Your Auth Token from twilio.com/console\n",
    "auth_token  = ''\n",
    "\n",
    "client = Client(account_sid, auth_token)\n",
    "\n",
    "message_body = 'NOAA MPSR update: there are %s new reports' %(new_reports)\n",
    "\n",
    "# Add from and to numbers below\n",
    "if new_reports >= 1:\n",
    "    message = client.messages.create(\n",
    "        to='', \n",
    "        from_='',\n",
    "        body=message_body)\n",
    "\n",
    "    print(message.sid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
